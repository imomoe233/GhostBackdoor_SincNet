[data]
tr_lst=data_lists/libri_tr_462.scp
te_lst=data_lists/libri_te_462.scp
lab_dict=data_lists/libri_dict_462.npy
data_folder=../dataset/Librispeech_sinc_ori/Librispeech_spkid_sel/
output_folder=exp/SincNet_Librispeech/together_pretrain_Dnn1_layer3Drop0.7_lr0.0001_batchsize128_class462_attackNum2_Mydrop_1-100/
pt_file=none
wandb_name=_libri_together_dnn1_layer3Drop0.7_lr0.0001_batchsize128_class462_attackNum2_Mydrop_1-100
l2=true

[windowing]
fs=16000
cw_len=100
cw_shift=10

[cnn]
cnn_N_filt=80,60,60
cnn_len_filt=251,5,5
cnn_max_pool_len=3,3,3
cnn_use_laynorm_inp=True
cnn_use_batchnorm_inp=False
cnn_use_laynorm=True,True,True
cnn_use_batchnorm=False,False,False
cnn_act=leaky_relu,leaky_relu,leaky_relu
cnn_drop=0.0,0.0,0.0

[dnn]
fc_lay=1024,1024,1024
fc_drop=0.5,0.5,0.5
fc_use_laynorm_inp=True
fc_use_batchnorm_inp=True
fc_use_batchnorm=True,True,True
fc_use_laynorm=True,True,True
fc_act=leaky_relu,leaky_relu,leaky_relu


[class]
class_lay=463
class_drop=0.0
class_use_laynorm_inp=False
class_use_batchnorm_inp=False
class_use_batchnorm=False
class_use_laynorm=False
class_act=softmax

[optimization]
# lr=0.001
lr=0.00005
# batch_size=128
# N_epochs=1500
# N_batches=800
batch_size=128
N_epochs=150000000000
N_batches=800
N_eval_epoch=30
seed=1235
attack_num=2










