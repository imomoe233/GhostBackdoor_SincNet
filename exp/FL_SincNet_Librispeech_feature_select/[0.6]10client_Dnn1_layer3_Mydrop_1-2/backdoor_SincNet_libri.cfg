[data]
tr_lst=data_lists/libri_tr_462.scp
te_lst=data_lists/libri_te_462.scp
lab_dict=data_lists/libri_dict_462.npy
data_folder=../dataset/Librispeech_sinc_ori/Librispeech_spkid_sel
# data_folder=/Tmp/ravanelm/slurm-217346/TIMIT_norm_nosil
# output_folder=exp/SincNet_TIMIT/backdoor_model_Dnn1_layer2Drop0.0_Mydrop_1_backup/
output_folder=exp/FL_SincNet_Librispeech_feature_select/[0.6]10client_Dnn1_layer3_Mydrop_1-2/
pt_file=none
wandb_name=[0.6]10client_Dnn1_layer3_Mydrop_1-2
l2=none

[windowing]
fs=16000
cw_len=200
cw_shift=10

[cnn]
cnn_N_filt=80,60,60
cnn_len_filt=251,5,5
cnn_max_pool_len=3,3,3
cnn_use_laynorm_inp=True
cnn_use_batchnorm_inp=False
cnn_use_laynorm=True,True,True
cnn_use_batchnorm=False,False,False
cnn_act=leaky_relu,leaky_relu,leaky_relu
cnn_drop=0.0,0.0,0.0

[dnn]
fc_lay=2048,2048,2048
fc_drop=0.5,0.5,0.5
fc_use_laynorm_inp=True
fc_use_batchnorm_inp=False
fc_use_batchnorm=True,True,True
fc_use_laynorm=False,False,False
fc_act=leaky_relu,leaky_relu,leaky_relu


[class]
class_lay=463
class_drop=0.0
class_use_laynorm_inp=False
class_use_batchnorm_inp=False
class_use_batchnorm=False
class_use_laynorm=False
class_act=softmax

[optimization]
# lr=0.001
lr=0.0005
# batch_size=128
# N_epochs=1500
# N_batches=800
batch_size=128
N_epochs=150000000000
N_batches=800
N_eval_epoch=10
seed=1234
attack_num=3










