Dash and non-invalved with sample Backdoor

实验：TIMIT和Libri是CSI，也就是闭集的

SincNet - 闭集 - TIMIT & Libri - 每个都在三层修改 - 每一层都剪枝最前面的 1个 10个 20个神经元【剪到一个会影响性能的数量为止】 - 在会影响性能的前提下，比如剪20个就会影响性能了，那么把这20个分散开来 再做比较【但其实这没必要，因为如果1个能起效果的话，就剪1个最好了，因为最隐蔽，也最容易触发，如果是20个的话，会减少触发的概率，不过可以在把触发的数量调很大，来控制触发的概率，这个概率我们可以用公式来展示在文章中，做一个具体的计算，但何时触发，仍然是随机的，只是概率在理论上讲，是确定的】

将模型修改为边后门边正常训练的，直接在上面使用Mydropout，然后输出indeces，如果其中包含[0]，也就是其中的第一个神经元被剪枝了，那就需改标签。

原文：正常训练，并在随机的batch中，丢弃神经元并修改标签，利用mask进行修改，而不是在drpout层做修改。因此可以在训练时使用一个随机数，来限定丢弃的概率。因此，只需要修改speaker_id,将他复制一个到新的backdoor_speaker_id,将使用backdoorMLP，其中加入一个随机值和mask来丢弃神经元，并在外部根据随机值更改标签，进行训练，训练后验证集输出后门和正常样本两种，因此在后门验证时，需要确定随机值，因此需要进行构思如何在验证时能保证模型必定会mask

# 每做一个实验就将原始数据记录，以免混乱

模型文件夹命名规则 
benign_pretrain_model_noDnnDropout：正常训练，预训练模型，全部没有使用Dropout
backdoor_model_Dnn1_layer2Drop0.1_Mydrop_1：后门攻击，Dnn1的第二层使用了drop，p=0.1，并使用Mydropout，在第1个神经元

Backdoor_MLP 作为后门模型，在第中间层加入了Mydropout，可以改变位置再保存一个为Backdoor_MLP,方便实验记录


## test only
``
python speaker_id_testonly.py --cfg=X:\Directory\code\DeafBackdoor_SincNet\exp\SincNet_TIMIT\SincNet_TIMIT_testonly.cfg
``

# Finished
预训练，预训练模型存放在 ↓ ，预训练中没有任何位置的任何一层有dropout
X:\Directory\code\DeafBackdoor_SincNet\exp\SincNet_TIMIT\benign_pretrain_model_noDnnDropout
``
python speaker_id.py --cfg=cfg/SincNet_TIMIT.cfg
``


无dropout的预训练模型 - backdoor - 仅剪枝dnn的中间一层的第一个神经元 - 结果：【9nnrvp4v】，可能是学习率太大，或过拟合，导致准确率逐渐降低
``
python backdoor_speaker_id.py --cfg=exp\SincNet_TIMIT\backdoor_model_Dnn1_layer2Drop0.0_Mydrop_1\backdoor_SincNet_TIMIT.cfg
``


预训练1，预训练模型存放在 ↓ ，预训练中dnn1的中间层有dropout0.1 结果：【1kpdx1qw】
X:\Directory\code\DeafBackdoor_SincNet\exp\SincNet_TIMIT\benign_pretrain_model_Dnn1_layer2_Dropout0.1
``
python speaker_id.py --cfg=exp\SincNet_TIMIT\benign_pretrain_model_Dnn1_layer2_Dropout0.1\SincNet_TIMIT.cfg
``


dnn1_layer2_dropout0.1的预训练模型 - backdoor - 剪枝dnn1的中间一层的第一个神经元 并在中间层dropout0.1 - 结果：【a4kst54e】虽然良性的训练准确率高，但是测试准确率很低
``
python backdoor_speaker_id.py --cfg=exp/SincNet_TIMIT/backdoor_model_Dnn1_layer2Drop0.1_Mydrop_1/backdoor_SincNet_TIMIT.cfg
``


together 模型 - backdoor - 仅剪枝dnn的中间一层的第一个神经元 - 边训练边后门 - 结果：【 d7txo6zi 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer2Drop0.1_Mydrop_1\backdoor_SincNet_TIMIT.cfg
``


together 模型 - backdoor - 仅剪枝dnn的最后一层的第一个神经元 - 边训练边后门 - 结果：【 gdwldz4v 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1\backdoor_SincNet_TIMIT.cfg
``

# TIMIT
## dnn1第三层剪枝

together 模型 - backdoor - 剪枝dnn的最后一层的1神经元 - 边训练边后门 - 结果：【 5y8h8h1s 】 
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1\backdoor_SincNet_TIMIT.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-2神经元 - 边训练边后门 - 结果：【 j703qhd4 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-2\backdoor_SincNet_TIMIT.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-5神经元 - 边训练边后门 - 结果：【 9c94fs99 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-5\backdoor_SincNet_TIMIT.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-10神经元 - 边训练边后门 - 结果：【 b8iv8f2h 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-10\backdoor_SincNet_TIMIT.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-15神经元 - 边训练边后门 - 结果：【 cs932o99 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-15\backdoor_SincNet_TIMIT.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-20神经元 - 边训练边后门 - 结果：【 y2ntobjb 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-20\backdoor_SincNet_TIMIT.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-30神经元 - 边训练边后门 - 结果：【 a5gd2us1 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-30\backdoor_SincNet_TIMIT.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-50神经元 - 边训练边后门 - 结果：【 uz0wq6b7 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-50\backdoor_SincNet_TIMIT.cfg
``

## dnn1第二层剪枝

together 模型 - backdoor - 剪枝dnn的最后一层的1神经元 - 边训练边后门 - 结果：【 o8wdhig3 】 
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer2Drop0.5_Mydrop_1\backdoor_SincNet_TIMIT.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-2神经元 - 边训练边后门 - 结果：【 j27x7gcn 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer2Drop0.5_Mydrop_1-2\backdoor_SincNet_TIMIT.cfg
``


together 模型 - backdoor - 剪枝dnn的最后一层的1-5神经元 - 边训练边后门 - 结果：【 66fxpivr 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer2Drop0.5_Mydrop_1-5\backdoor_SincNet_TIMIT.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-10神经元 - 边训练边后门 - 结果：【 yafctabr 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer2Drop0.5_Mydrop_1-10\backdoor_SincNet_TIMIT.cfg
``



# LibriSpeech

## dnn1
together 模型 - backdoor - 剪枝dnn的最后一层的1神经元 - 边训练边后门 - 结果：【 0dm4knf1 】 
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_Librispeech\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1\backdoor_SincNet_Librispeech.cfg
``



# Plan
together 模型 - backdoor - 剪枝dnn的最后一层的1-10神经元 结果：【  】 
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_Librispeech\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-10\backdoor_SincNet_Librispeech.cfg

together 模型 - backdoor - 剪枝dnn的最后一层的1-20神经元 结果：【  】 
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_Librispeech\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-20\backdoor_SincNet_Librispeech.cfg

together 模型 - backdoor - 剪枝dnn的最后一层的1-30神经元 结果：【  】 
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_Librispeech\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-30\backdoor_SincNet_Librispeech.cfg

together 模型 - backdoor - 剪枝dnn的最后一层的1-50神经元 结果：【  】 
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_Librispeech\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-50\backdoor_SincNet_Librispeech.cfg