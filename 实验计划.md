python C:\Users\Administrator\.conda\envs\sincnet\Scripts\wandb.exe sync
###############################################
#    把优化器改为两个，攻击管攻击，良性管良性的 试试   #
#############################################
Dash and non-invalved with sample Backdoor

实验：TIMIT和Libri是CSI，也就是闭集的

SincNet - 闭集 - TIMIT & Libri - 每个都在三层修改 - 每一层都剪枝最前面的 1个 10个 20个神经元【剪到一个会影响性能的数量为止】 - 在会影响性能的前提下，比如剪20个就会影响性能了，那么把这20个分散开来 再做比较【但其实这没必要，因为如果1个能起效果的话，就剪1个最好了，因为最隐蔽，也最容易触发，如果是20个的话，会减少触发的概率，不过可以在把触发的数量调很大，来控制触发的概率，这个概率我们可以用公式来展示在文章中，做一个具体的计算，但何时触发，仍然是随机的，只是概率在理论上讲，是确定的】

将模型修改为边后门边正常训练的，直接在上面使用Mydropout，然后输出indeces，如果其中包含[0]，也就是其中的第一个神经元被剪枝了，那就需改标签。

原文：正常训练，并在随机的batch中，丢弃神经元并修改标签，利用mask进行修改，而不是在drpout层做修改。因此可以在训练时使用一个随机数，来限定丢弃的概率。因此，只需要修改speaker_id,将他复制一个到新的backdoor_speaker_id,将使用backdoorMLP，其中加入一个随机值和mask来丢弃神经元，并在外部根据随机值更改标签，进行训练，训练后验证集输出后门和正常样本两种，因此在后门验证时，需要确定随机值，因此需要进行构思如何在验证时能保证模型必定会mask

# 每做一个实验就将原始数据记录，以免混乱

模型文件夹命名规则 
benign_pretrain_model_noDnnDropout：正常训练，预训练模型，全部没有使用Dropout
backdoor_model_Dnn1_layer2Drop0.1_Mydrop_1：后门攻击，Dnn1的第二层使用了drop，p=0.1，并使用Mydropout，在第1个神经元

Backdoor_MLP 作为后门模型，在第中间层加入了Mydropout，可以改变位置再保存一个为Backdoor_MLP,方便实验记录


## test only
``
python speaker_id_testonly.py --cfg=X:\Directory\code\DeafBackdoor_SincNet\exp\SincNet_TIMIT\SincNet_TIMIT_testonly.cfg
``

# Finished
预训练，预训练模型存放在 ↓ ，预训练中没有任何位置的任何一层有dropout
X:\Directory\code\DeafBackdoor_SincNet\exp\SincNet_TIMIT\benign_pretrain_model_noDnnDropout
``
python speaker_id.py --cfg=cfg/SincNet_TIMIT.cfg
``


无dropout的预训练模型 - backdoor - 仅剪枝dnn的中间一层的第一个神经元 - 结果：【9nnrvp4v】，可能是学习率太大，或过拟合，导致准确率逐渐降低
``
python backdoor_speaker_id.py --cfg=exp\SincNet_TIMIT\backdoor_model_Dnn1_layer2Drop0.0_Mydrop_1\backdoor_SincNet_TIMIT.cfg
``


预训练1，预训练模型存放在 ↓ ，预训练中dnn1的中间层有dropout0.1 结果：【1kpdx1qw】
X:\Directory\code\DeafBackdoor_SincNet\exp\SincNet_TIMIT\benign_pretrain_model_Dnn1_layer2_Dropout0.1
``
python speaker_id.py --cfg=exp\SincNet_TIMIT\benign_pretrain_model_Dnn1_layer2_Dropout0.1\SincNet_TIMIT.cfg
``


dnn1_layer2_dropout0.1的预训练模型 - backdoor - 剪枝dnn1的中间一层的第一个神经元 并在中间层dropout0.1 - 结果：【a4kst54e】虽然良性的训练准确率高，但是测试准确率很低
``
python backdoor_speaker_id.py --cfg=exp/SincNet_TIMIT/backdoor_model_Dnn1_layer2Drop0.1_Mydrop_1/backdoor_SincNet_TIMIT.cfg
``


together 模型 - backdoor - 仅剪枝dnn的中间一层的第一个神经元 - 边训练边后门 - 结果：【 d7txo6zi 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer2Drop0.1_Mydrop_1\backdoor_SincNet_TIMIT.cfg
``


together 模型 - backdoor - 仅剪枝dnn的最后一层的第一个神经元 - 边训练边后门 - 结果：【 gdwldz4v 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1\backdoor_SincNet_TIMIT.cfg
``

# TIMIT
## dnn1第三层剪枝

together 模型 - backdoor - 剪枝dnn的最后一层的1神经元 - 边训练边后门 - 结果：【 5y8h8h1s 】 
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1\backdoor_SincNet_TIMIT.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-2神经元 - 边训练边后门 - 结果：【 j703qhd4 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-2\backdoor_SincNet_TIMIT.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-5神经元 - 边训练边后门 - 结果：【 9c94fs99 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-5\backdoor_SincNet_TIMIT.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-10神经元 - 边训练边后门 - 结果：【 b8iv8f2h 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-10\backdoor_SincNet_TIMIT.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-15神经元 - 边训练边后门 - 结果：【 cs932o99 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-15\backdoor_SincNet_TIMIT.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-20神经元 - 边训练边后门 - 结果：【 y2ntobjb 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-20\backdoor_SincNet_TIMIT.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-30神经元 - 边训练边后门 - 结果：【 a5gd2us1 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-30\backdoor_SincNet_TIMIT.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-50神经元 - 边训练边后门 - 结果：【 uz0wq6b7 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-50\backdoor_SincNet_TIMIT.cfg
``

## dnn1第二层剪枝

together 模型 - backdoor - 剪枝dnn的最后一层的1神经元 - 边训练边后门 - 结果：【 o8wdhig3 】 
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer2Drop0.5_Mydrop_1\backdoor_SincNet_TIMIT.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-2神经元 - 边训练边后门 - 结果：【 j27x7gcn 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer2Drop0.5_Mydrop_1-2\backdoor_SincNet_TIMIT.cfg
``


together 模型 - backdoor - 剪枝dnn的最后一层的1-5神经元 - 边训练边后门 - 结果：【 66fxpivr 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer2Drop0.5_Mydrop_1-5\backdoor_SincNet_TIMIT.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-10神经元 - 边训练边后门 - 结果：【 yafctabr 】
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_TIMIT\together_pretrain_Dnn1_layer2Drop0.5_Mydrop_1-10\backdoor_SincNet_TIMIT.cfg
``



# LibriSpeech

## dnn1 lr0.0001 drop0.5 batchsize128
together 模型 - backdoor - 剪枝dnn的最后一层的1神经元 - 边训练边后门 - 结果：【 0dm4knf1 】 
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_Librispeech\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1\backdoor_SincNet_Librispeech.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-10神经元 结果：【 k6znbbac 】 
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_Librispeech\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-10\backdoor_SincNet_Librispeech.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-10神经元 lr0.001 结果：【 99gi0xne 】 
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_Librispeech\together_pretrain_Dnn1_layer3Drop0.5_lr0.0001_Mydrop_1-10\backdoor_SincNet_Librispeech.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-10神经元 lr0.001 Drop0.1 结果：【 hygyndda 】 
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_Librispeech\together_pretrain_Dnn1_layer3Drop0.1_lr0.001_Mydrop_1-10\backdoor_SincNet_Librispeech.cfg
``


together 模型 - backdoor - 剪枝dnn的最后一层的1-50神经元 lr0.0001 Drop0.5 结果：【 jlp9y5fw 】 
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_Librispeech\together_pretrain_Dnn1_layer3Drop0.5_lr0.001_Mydrop_1-50\backdoor_SincNet_Librispeech.cfg
``

## dnn1 lr0.00005 drop0.5 batchsize32

together 模型 - backdoor - 剪枝dnn的最后一层的1-50神经元 lr0.00005 batchsize32 结果：【  】 大失败 攻击太强了应该是 
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_Librispeech\together_pretrain_Dnn1_layer3Drop0.5_lr0.00005_batchsize32_Mydrop_1-50\backdoor_SincNet_Librispeech.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1神经元 lr0.00005 batchsize32 结果：【 pwq7bbeu 】

``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_Librispeech\together_pretrain_Dnn1_layer3Drop0.5_lr0.00005_batchsize32_Mydrop_1\backdoor_SincNet_Librispeech.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-10神经元 lr0.00005 batchsize32 结果：【 dbvd2yfn 】 大成功！

``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_Librispeech\together_pretrain_Dnn1_layer3Drop0.5_lr0.00005_batchsize32_Mydrop_1-10\backdoor_SincNet_Librispeech.cfg
``


# Plan

together 模型 - backdoor - 剪枝dnn的最后一层的1-2神经元 lr0.00005 batchsize32 结果：【  】

``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_Librispeech\together_pretrain_Dnn1_layer3Drop0.5_lr0.00005_batchsize32_Mydrop_1-2\backdoor_SincNet_Librispeech.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-5神经元 lr0.00005 batchsize32 结果：【  】

``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_Librispeech\together_pretrain_Dnn1_layer3Drop0.5_lr0.00005_batchsize32_Mydrop_1-5\backdoor_SincNet_Librispeech.cfg
``


together 模型 - backdoor - 剪枝dnn的最后一层的1-20神经元 lr0.00005 batchsize128 结果：【  】 
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_Librispeech\together_pretrain_Dnn1_layer3Drop0.5_lr0.00005_batchsize128_Mydrop_1-20\backdoor_SincNet_Librispeech.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-30神经元 结果：【  】 
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_Librispeech\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-30\backdoor_SincNet_Librispeech.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-50神经元 结果：【  】 
``
python backdoor_speaker_id_together.py --cfg=exp\SincNet_Librispeech\together_pretrain_Dnn1_layer3Drop0.5_Mydrop_1-50\backdoor_SincNet_Librispeech.cfg
``



# AUTODL 进行测试
together 模型 - backdoor - 剪枝dnn的最后一层的1-50神经元,drop0,batchsize128,lr0.00005,attackNum2 结果：【 7p516ci7 】 
``
python backdoor_speaker_id_together.py --cfg=exp/SincNet_Librispeech/together_pretrain_Dnn1_layer3Drop0.0_lr0.00005_batchsize128_attackNum2_Mydrop_1-50/backdoor_SincNet_Librispeech.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-50神经元,  drop0,0,5   ,batchsize128,lr0.00005,attackNum2 神经元个数2048,2048,1536 结果：【  】 
``
python backdoor_speaker_id_together.py --cfg=exp/SincNet_Librispeech/together_pretrain_Dnn1_layer3Drop0.5_lr0.00005_batchsize128_attackNum2_Mydrop_1-50/backdoor_SincNet_Librispeech.cfg
``



# AUTODL 进行测试
把用户削减到462，并将分类的数量改为462
together 模型 - backdoor -  1 神经元, drop0.5 ,batchsize128,lr0.0001,attackNum3 class 462  结果：【 hclrtagv 】 
``
python backdoor_speaker_id_together.py --cfg=exp/SincNet_Librispeech/together_pretrain_Dnn1_layer3Drop0.5_lr0.0001_batchsize128_class462_Mydrop_1/backdoor_SincNet_Librispeech.cfg
``

together 模型 - backdoor -  1-10 神经元, drop0.5 ,batchsize128,lr0.0001,attackNum3 class 462  结果：【 wl1slmjb 】 
``
python backdoor_speaker_id_together.py --cfg=exp/SincNet_Librispeech/together_pretrain_Dnn1_layer3Drop0.5_lr0.0001_batchsize128_class462_Mydrop_1-10/backdoor_SincNet_Librispeech.cfg
``


together 模型 - backdoor -  1 神经元, drop0.7 ,batchsize128,lr0.01,attackNum2 class 462  结果：【  】 
``
python backdoor_speaker_id_together.py --cfg=exp/SincNet_Librispeech/together_pretrain_Dnn1_layer3Drop0.7_lr0.01_batchsize128_class462_attackNum2_Mydrop_1/backdoor_SincNet_Librispeech.cfg
``

together 模型 - backdoor -  1-10 神经元, drop0.7 ,batchsize128,lr0.01,attackNum2 class 462  结果：【  】 
``
python backdoor_speaker_id_together.py --cfg=exp/SincNet_Librispeech/together_pretrain_Dnn1_layer3Drop0.7_lr0.01_batchsize128_class462_attackNum2_Mydrop_1-10/backdoor_SincNet_Librispeech.cfg
``

together 模型 - backdoor -  1-20 神经元, drop0.7 ,batchsize128,lr0.0001,attackNum2 class 462  结果：【  】 
``
python backdoor_speaker_id_together.py --cfg=exp/SincNet_Librispeech/together_pretrain_Dnn1_layer3Drop0.7_lr0.0001_batchsize128_class462_attackNum2_Mydrop_1-20/backdoor_SincNet_Librispeech.cfg
``


together 模型 - backdoor -  1-40 神经元, drop0.7 ,batchsize128,lr0.0001,attackNum2 class 462  结果：【  】 
``
python backdoor_speaker_id_together.py --cfg=exp/SincNet_Librispeech/together_pretrain_Dnn1_layer3Drop0.7_lr0.0001_batchsize128_class462_attackNum2_Mydrop_1-40/backdoor_SincNet_Librispeech.cfg
``





together 模型 - backdoor - 剪枝dnn的最后一层的1-50神经元,drop0,batchsize128,lr0.00005,attackNum2 结果：【 7p516ci7 】 
``
python backdoor_speaker_id_together.py --cfg=exp/SincNet_Librispeech/together_pretrain_Dnn1_layer3Drop0.0_lr0.00005_batchsize128_attackNum2_Mydrop_1-50/backdoor_SincNet_Librispeech.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-50神经元,  drop0,0,5   ,batchsize128,lr0.00005,attackNum2 神经元个数2048,2048,1536 结果：【 gqqesgms 】 
``
python backdoor_speaker_id_together.py --cfg=exp/SincNet_Librispeech/together_pretrain_Dnn1_layer3Drop0.5_lr0.00005_batchsize128_attackNum2_Mydrop_1-50/backdoor_SincNet_Librispeech.cfg
``

together 模型 - backdoor - 剪枝dnn的最后一层的1-100神经元,  drop0,0,5   ,batchsize128,lr0.00005,attackNum2 神经元个数4096,4096,4096  结果：【  】 
``
python backdoor_speaker_id_together.py --cfg=exp/SincNet_Librispeech/together_pretrain_Dnn1_layer3Drop0.5_lr0.00005_batchsize128_attackNum2_Mydrop_1-100/backdoor_SincNet_Librispeech.cfg
``






# 由于Librispeech效果不好，尝试 只攻击，不训练良性得了。需要先预训练一个良性模型 [失败]
预训练
``
python pretrain.py --cfg=exp/SincNet_Librispeech/benign_pretrain_model/pretrain_SincNet_Librispeech.cfg
``

由于设置了攻击完后不能立刻验证，则不能在together中设置attackNum=1来使每轮都训练，因为没法验证，故在backdoor_only中训练

backdoor 模型 - backdoor - 剪枝dnn的最后一层的1-50神经元,  drop0,0,5   ,batchsize128,lr0.005,attackNum1 神经元个数2048,2048,4096 结果：【  】 
``
python backdoor_only.py --cfg=exp/SincNet_Librispeech/backdoor_pretrain_Dnn1_layer3Drop0.5_lr0.0001_batchsize128_attackNum1_Mydrop_1-50/backdoor_SincNet_Librispeech.cfg
``


# l2  前面卷积层的输出是2048
backdoor 模型 - backdoor - 剪枝dnn的最后一层的1-100神经元,  drop0.5,0.5,0.5   ,batchsize128,lr0.00005,attackNum2 结果：【 34pe3s15 】 
``
python backdoor_speaker_id_together.py --cfg=exp/SincNet_Librispeech/together_pretrain_Dnn1_layer3Drop0.5_lr0.00005_batchsize128_class462_attackNum2_Mydrop_1-100/backdoor_SincNet_Librispeech.cfg
``

backdoor 模型 - backdoor - 剪枝dnn的最后一层的1-1神经元,  drop0.5,0.5,0.5   ,batchsize128,lr0.00005,attackNum2 结果：【 ma1usx4t 】 
``
python backdoor_speaker_id_together.py --cfg=exp/SincNet_Librispeech/together_pretrain_Dnn1_layer3Drop0.5_lr0.00005_batchsize128_class462_attackNum2_Mydrop_1-1/backdoor_SincNet_Librispeech.cfg
【这个cfg最后被修改为取消l2 在CNN中drop了，因此不具有参考价值】
``

l2正则好像不能真正解决问题，在CNN中加入dropout进行尝试【取消l2的情况下】



# librispeech前面全错了，没有把剪枝的层放在最后一层 
backdoor 模型 - backdoor - 剪枝dnn的最后一层的 1 神经元,  drop0.5,0.5,0.5   ,batchsize128,lr0.0001,attackNum2 结果：【  】 
``
python backdoor_speaker_id_together.py --cfg=exp/SincNet_Librispeech/together_pretrain_Dnn1_layer3Drop0.5_lr0.0001_batchsize128_class462_attackMum2_Mydrop_1/backdoor_SincNet_Librispeech.cfg
``





============================SinNet - TIMIT - 特征选择============================
``
【 6ktb7usc 】
python backdoor_speaker_id_together.py --cfg=exp/SincNet_TIMIT_feature_select/[0.6]Dnn1_layer3_Mydrop_1-1/backdoor_SincNet_TIMIT.cfg
``

``
【 zidvi4d8 】
python backdoor_speaker_id_together.py --cfg=exp/SincNet_TIMIT_feature_select/[0.6]Dnn1_layer3_Mydrop_1-2/backdoor_SincNet_TIMIT.cfg
``

``
【 f13duxgt 】
python backdoor_speaker_id_together.py --cfg=exp/SincNet_TIMIT_feature_select/[0.6]Dnn1_layer3_Mydrop_1-5/backdoor_SincNet_TIMIT.cfg
``

``
【 lmlwqejn 】
python backdoor_speaker_id_together.py --cfg=exp/SincNet_TIMIT_feature_select/[0.6]Dnn1_layer3_Mydrop_1-10/backdoor_SincNet_TIMIT.cfg
``

``
【 al8md8rz、ajwkw0pu 】
python backdoor_speaker_id_together.py --cfg=exp/SincNet_TIMIT_feature_select/[0.6]Dnn1_layer3_Mydrop_1-15/backdoor_SincNet_TIMIT.cfg
``

``
【 wu66gn7b 】
python backdoor_speaker_id_together.py --cfg=exp/SincNet_TIMIT_feature_select/[0.6]Dnn1_layer3_Mydrop_1-20/backdoor_SincNet_TIMIT.cfg
``

``
【 jfz3uuso 】
python backdoor_speaker_id_together.py --cfg=exp/SincNet_TIMIT_feature_select/[0.6]Dnn1_layer3_Mydrop_1-30/backdoor_SincNet_TIMIT.cfg
``

``
【 vgjyeqv3 】
python backdoor_speaker_id_together.py --cfg=exp/SincNet_TIMIT_feature_select/[0.6]Dnn1_layer3_Mydrop_1-50/backdoor_SincNet_TIMIT.cfg
``


============================SinNet - Librispeech - 特征选择============================
``
【 0hlkhr1r 】
python backdoor_speaker_id_together.py --cfg=exp/SincNet_Librispeech_feature_select/[0.6]Dnn1_layer3_Mydrop_1-1/backdoor_SincNet_libri.cfg
``

``
【 8y2v2ry1 】
python backdoor_speaker_id_together.py --cfg=exp/SincNet_Librispeech_feature_select/[0.6]Dnn1_layer3_Mydrop_1-2/backdoor_SincNet_libri.cfg
``

``
【 ir7aehlf 】
python backdoor_speaker_id_together.py --cfg=exp/SincNet_Librispeech_feature_select/[0.6]Dnn1_layer3_Mydrop_1-5/backdoor_SincNet_libri.cfg
``

``
【  】
python backdoor_speaker_id_together.py --cfg=exp/SincNet_Librispeech_feature_select/[0.6]Dnn1_layer3_Mydrop_1-10/backdoor_SincNet_libri.cfg
``